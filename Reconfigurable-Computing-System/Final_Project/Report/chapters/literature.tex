\فصل{طراحی و شبیه‌سازی}\label{طراحی و شبیه‌سازی}


هدف در این پروژه پیاده سازی یک شبکه \lr{CNN} بر روی \lr{FPGA} با استفاده از ابزار \lr{HLS} با هدف تشخیص ارقام دست نویس است. بدین منظور پروژه را به دو فاز تقسیم می‌کنیم:
\begin{enumerate}
	\item فاز نرم‌افزاری
	\item فاز سخت‌افزاری
\end{enumerate}

که در ادامه هر یک از بخش‌های پروژه را با جزئیات توضیح می‌دهیم.



\قسمت{فاز نرم‌افزاری}

\زیرقسمت{انتخاب معماری}
در ابتدا می‌بایست بهترین معماری را برای شبکه خود انتخاب کنیم. معیار‌هایی که در انتخاب معماری به آن توجه شده است به‌صورت زیر می‌باشد:

\شروع{فقرات}

\فقره دقت\پانویس{\lr{Accuracy}} شبکه
\فقره خطا\پانویس{\lr{Loss}} شبکه
\فقره حجم پارامتر‌های ذخیره شده

\پایان{فقرات}

در صورت پروژه بیان شده است که دقت آموزش شبکه می‌بایست بالای ۹۰ درصد باشد. همچنین به دلیل آنکه قرار است پارامتر‌های آموزش دیده را در فاز دوم به \lr{HLS} ببریم و در آنجا بارگزاری کنیم یکی از مواردی که بسیار برای ما اهمیت دارد حجم پارامتر های ذخیره شده در مدل است. بنابر این با توجه به این سه معیار مطرح شده مدل های مختلفی را تست کرده ایم که در جدول زیر آورده شده است. \\


	
\begin{table}[ht]
	\centering
	\caption{معماری‌های بررسی شده}
	\label{جدول:جدول معماری ها}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{ c c c c c c c c c }
			\hline
			\multirow{2}{*}{\textbf{معماری}} & \multirow{2}{*}{\textbf{لایه \lr{Zero Pading}}} & \multirow{2}{*}{\textbf{لایه \lr{Max Pooling}}} & \multirow{2}{*}{\textbf{تعداد لایه‌های کانولوشن}} & \multirow{2}{*}{\textbf{تعداد فیلترها}} & \multirow{2}{*}{\textbf{اندازه کرنل}} & \multirow{2}{*}{\textbf{\lr{Stride}}} & \multirow{2}{*}{\textbf{تابع فعالساز}} & \multirow{2}{*}{\textbf{تعداد لایه‌های \lr{Fully Connected}}} \\
			& & & & & & & & \\ \hline\hline
			\textbf{معماری 1} & بله & خیر & 1 & 8 & 3x3 & 1 & \lr{ReLU} & 4 (10, 64, 64, 128) \\ \hline
			\textbf{معماری 2} & بله & خیر & 1 & 8 & 3x3 & 1 & \lr{ReLU} & 3 (10, 64, 128) \\ \hline
			\textbf{معماری 3} & بله & خیر & 1 & 8 & 3x3 & 1 & \lr{ReLU} & 2 (10, 128) \\ \hline
			\textbf{معماری 4} & بله & خیر & 1 & 8 & 3x3 & 1 & \lr{ReLU} & 1 (10) \\ \hline
			\textbf{معماری 5} & بله & بله & 1 & 4 & 7x7 & 1 & \lr{ReLU} & 1 (10) \\ \hline\hline
		\end{tabular}
	}
\end{table}



به‌ازای تمامی معماری های جدول \ref{جدول:جدول معماری ها} یک بار شبکه آموزش داده شده است و تعداد پارامتر‌های مدل و حجم آن‌ها به‌صورت زیر گزارش می‌شود:\\

\begin{table}[ht]
	\centering
	\caption{اطلاعات مربوط به معماری‌های مختلف}
	\resizebox{0.8\textwidth}{!}{ % Resize the table to 80% of the text width
		\begin{tabular}{ c c c c c c }
			\hline
			\textbf{معماری} & \textbf{تعداد کل پارامتر‌ها} & \textbf{حجم مصرفی} & \textbf{دقت داده‌های آموزش} & \textbf{دقت داده‌های تست} \\ \hline \hline
			\textbf{معماری 1} & 934874 & \lr{MB} ۳ & ۹۹۲۶٫۰ & ۹۷۷۶٫۰ \\ \hline
			\textbf{معماری 2} & 930714 & \lr{MB} ۵۵٫۳ & ۹۹۵۹٫۰ & ۹۷۶۰٫۰ \\ \hline
			\textbf{معماری 3} & 923098 & \lr{MB} ۵۲٫۳ & ۹۹۷۰٫۰ & ۹۸۰۸٫۰ \\ \hline
			\textbf{معماری 4} & 72090 & \lr{KB} ۶۰٫۲۸۱ & ۹۸۵۷٫۰ & ۹۷۵۹٫۰ \\ \hline
			\textbf{معماری 5} & 8050 & \lr{KB} 45.31 & ۹۷۷۹٫۰ & ۹۷۵۸٫۰ \\ \hline\hline
		\end{tabular}
	}
\end{table}


بنابر این طبق نتایج به‌دست آمده، معماری شماره ۵ را به‌عنوان معماری برگزیده انتخاب می‌کنیم.

معماری نهایی شبکه به‌صورت زیر ارائه می‌شود:\newpage

\شروع{شکل}[ht]
\centerimg{img8}{9cm}
\شرح{معماری شبکه}
\برچسب{شکل:معماری شبکه}
\پایان{شکل}


\زیرقسمت{آموزش شبکه}
پس از انتخاب معماری شبکه، نوبت به آموزش شبکه می‌رسد. به‌صورت نرم افزاری شبکه مورد نظر را تعریف کرده و آن را با داده‌های مجموعه داده \lr{MNIST} آموزش می‌دهیم تا از وزن‌های آن در فاز سخت افزاری استفاده کنیم.


کد نوشته شده برای پیاده سازی شبکه به‌صورت زیر است: \newpage


\begin{latin}
\begin{lstlisting}[language=Python,caption={Model Definition}]
	
def define_model() -> Sequential:
	# Define model.
	model = Sequential()
	model.add(ZeroPadding2D(padding=pad, input_shape=(input_size[0],
		input_size[1], 1), name="padding_layer"))
	model.add(Conv2D(conv_filter_num, conv_kernel_size, activation="relu",
		padding="valid", kernel_initializer="he_uniform",
		input_shape=(30, 30, 1), name="convolution_layer"))
	model.add(MaxPooling2D(pool_size, name="max_pooling_layer"))
	model.add(Flatten(name="flatten_layer"))
	model.add(Dense(10, activation="softmax", name="dense_layer"))
	
	# Compile model.
	model.Compile(optimizer=Adam(), loss="categorical_crossentropy",
		metrics=["accuracy"])
		
	# Return model.
	return model
	
\end{lstlisting}
\end{latin}



با کامپایل کردن مدل نوشته شده خروجی شبکه تعریف شده به‌صورت زیر می‌شود:


\شروع{شکل}[ht]
\centerimg{img9}{12cm}
\شرح{ساختار شبکه تعریف شده}
\برچسب{شکل:ساختار شبکه تعریف شده}
\پایان{شکل}



پس از تعریف ساختار شبکه، شبکه را با استفاده از دیتاست \lr{MNIST} در ۵ \lr{Epoch} آموزش می‌دهیم. که نمودار \lr{Loss} و \lr{Accuracy} شبکه برای داده‌های \lr{Train} و \lr{Validation} به‌صورت شکل «\ref{شکل:نمودارهای Loss و Accuracy}» به‌دست می‌آید:


\شروع{شکل}[ht]
\centerimg{img10}{12cm}
\شرح{نمودارهای \lr{Loss} و \lr{Accuracy}}
\برچسب{شکل:نمودارهای Loss و Accuracy}
\پایان{شکل}



دقت شبکه بر روی داده‌های آموزش و تست به ترتیب ۹۷۷۹٫۰ و ۹۷۵۸٫۰ به‌دست آمده است.

همچنین زمان انجام فاز \lr{Inference} نرم‌افزاری برای ۱۰۰ داده از مجموعه داده \lr{MNIST}، ۹۷۳۲٫۳۹ میلی‌ثانیه به‌دست آمده است.

\زیرقسمت{ذخیره پارامترها}
در نهایت تمامی وزن‌ها و پارامتر‌های مورد نیاز شبکه برای فاز سخت‌افزاری را در ۳ فایل با نام‌های:

\begin{latin}
	\begin{itemize}
		\item 
		\texttt{conv\_weights.h}
		
		\item 
		\texttt{dense\_weights.h}
		
		\item 
		\texttt{definitions.h}
	\end{itemize}
\end{latin}


ذخیره می‌کنیم. فایل \texttt{conv\_weights.h} شامل وزن‌های به‌دست آمده از لایه‌های کانولوشن، فایل \texttt{dense\_weights.h} نیز شامل وزن‌های لایه \lr{Fully Connected} و فایل \texttt{definitions.h} شامل برخی از ضرایب ثابت مورد استفاده در فاز سخت‌افزاری است.

در ادامه تمامی داده‌های تست \lr{MNIST} شامل تصاویر و \lr{Label} های مربوط به آن را در دو فایل \texttt{in.dat} و \texttt{out.dat} ذخیره می‌کنیم. مراحل انجام به طور کامل در فایل \texttt{gen\_data.ipynb} مشخص شده است.





\قسمت{فاز سخت‌افزاری}

در این فاز نیاز است کد های همه لایه‌ها را به‌صورت سخت‌افزاری در \lr{HLS} نوشته و با همان معماری فاز نرم‌افزاری پیاده‌سازی کنیم.

\زیرقسمت{\lr{Fix-Point} کردن داده‌ها}
قبل از هرچیزی نیاز است وزن‌های \lr{Floating-Point} ذخیره شده از فاز نرم‌افزاری را کاهش منابع مصرفی، به \lr{Fix-Point} تبدیل کنیم. بدین منظور از نوع داده \texttt{ap\_fixed} موجود در کتابخانه \texttt{ap\_fixed.h} استفاده می‌کنیم.


این نوع داده به‌صورت زیر استفاده می‌شود:

\begin{latin}
	\texttt{ap\_fixed<WL, IWL>}
\end{latin}

که:

\شروع{فقرات}

\فقره \texttt{WL} تعداد کل بیت‌ها (کل عرض داده).

\فقره \texttt{IWL} تعداد بیت‌های بخش صحیح داده.

\پایان{فقرات}

به عنوان مثال می‌توان نوشت:

\begin{latin}
	\texttt{ap\_fixed<16, 8> my\_number;}
\end{latin}

\شروع{فقرات}

\فقره \texttt{:16} طول کل بیت‌ها (بخش صحیح + اعشاری).

\فقره \texttt{:8} تعداد بیت‌های بخش صحیح.

\پایان{فقرات}


\زیرقسمت{لایه کانولوشن}
برای مثال کد لایه کانولوشن به‌صورت زیر نوشته شده است:




\begin{latin}
\begin{lstlisting}[language=C,caption={HLS Implementation of Convolution Layer}]

void convolution( float pad_img [PAD_IMG_ROWS][PAD_IMG_COLS],
				  int filter,
				  hls::stream<float> & conv_to_pool_stream )
{
	float w_sum = 0.0; // Weighted sum.
	
	// outer loops (r and c) loop over all pooling regions
	conv_for_rows: for(int r = 0; r < IMG_ROWS; r += POOL_ROWS)
	{
		conv_for_cols: for(int c = 0; c < IMG_COLS; c += POOL_COLS)
		{
			// middle loops (pr and pc) loop over all pixels
			// in selected pooling region
			pool_for_rows: for(int pr = 0; pr < POOL_ROWS; ++pr)
			{
				pool_for_cols: for(int pc = 0; pc < POOL_COLS; ++pc)
				{
					w_sum = 0.0;
					
					// inner loops (kr and kc) loop over all filter coefficients
					// applied to neighborhood of selected pixel
					krn_for_rows: for(int kr = 0; kr < KRN_ROWS; ++kr)
					{
						krn_for_cols: for(int kc = 0; kc < KRN_COLS; ++kc)
						{
							float w     = conv_weights[filter][kr][kc];
							float pixel = pad_img[r+pr+kr][c+pc+kc];
							w_sum +=  w * pixel;
						}
					}
					
					conv_to_pool_stream.write(relu(w_sum + conv_biases[filter]));
				}
			}
		}
	}
}
\end{lstlisting}
\end{latin}

در این طراحی، کانولوشن چند کاناله را به‌صورت چند لایه کانولوشن تک کاناله پیاده سازی کرده‌ایم. در فاز نرم افزاری یک لایه کانولوشن ۴ کاناله داشتیم اما برای راحتی پیاده سازی، در فاز سخت‌افزاری، آن را به ۴ لایه کانولوشن تک کاناله شکسته ایم و آن را به‌صورت زیر پیاده‌سازی کرده‌ایم:



\begin{latin}
	\begin{lstlisting}[language=C,caption={HLS Implementation of Convolution Layers}]
		
		void convolutional_layer(float pad_img0 [PAD_IMG_ROWS][PAD_IMG_COLS],
		float pad_img1 [PAD_IMG_ROWS][PAD_IMG_COLS],
		float pad_img2 [PAD_IMG_ROWS][PAD_IMG_COLS],
		float pad_img3 [PAD_IMG_ROWS][PAD_IMG_COLS],
		hls::stream<float> conv_to_pool_streams [FILTERS] )
		{
			convolution(pad_img0, 0, conv_to_pool_streams[0]);
			convolution(pad_img1, 1, conv_to_pool_streams[1]);
			convolution(pad_img2, 2, conv_to_pool_streams[2]);
			convolution(pad_img3, 3, conv_to_pool_streams[3]);
		}
	\end{lstlisting}
\end{latin}




\زیرقسمت{لایه \lr{Fully Connected}}
کد لایه \lr{Fully Connected} نیز به‌صورت زیر نوشته شده است:

\begin{latin}
\begin{lstlisting}[language=C,caption={HLS Implementation of Dense Layer}]
	
void dense( hls::stream<float> & flat_to_dense_stream,
			int filter,
			hls::stream<float> & dense_to_softmax_stream )
{
	float flat_value;
	float dense_array[DENSE_SIZE] = { 0 };
	
	
	#pragma HLS ARRAY_PARTITION variable=dense_array complete
	#pragma HLS PIPELINE II=1
	
	dense_for_flat: for(int i = 0; i < FLAT_SIZE / FILTERS; ++i)
	{
		flat_value = flat_to_dense_stream.read();
		
		for(int d = 0; d < DENSE_SIZE; ++d)
		{
			int index = filter * (FLAT_SIZE / FILTERS) + i;
			dense_array[d] += dense_weights[index][d] * flat_value;
		}
	}
	
	for(int j = 0; j < DENSE_SIZE; ++j)
	{
		dense_to_softmax_stream.write(dense_array[j]);
	}
}
\end{lstlisting}
\end{latin}



\زیرقسمت{لایه \lr{Flatten}}
همچنین برای \lr{Flat} کردن \lr{Feature Map} های به‌دست آمده از خروجی لایه‌های کانولوشنی، ماژولی به‌نام \texttt{flattening} به‌صورت زیر تعریف شده است:

\begin{latin}
\begin{lstlisting}[language=C,caption={HLS Implementation of Flatten Layers}]

void flattening( hls::stream<float> &  pool_to_flat_stream,
				 hls::stream<float> &  flat_to_dense_stream )
{
	
	#pragma HLS ARRAY_PARTITION variable=pool_to_flat_stream complete
	#pragma HLS ARRAY_PARTITION variable=flat_to_dense_stream complete
	
	flat_for_rows: for(int r = 0; r < POOL_IMG_ROWS; ++r)
	{
		flat_for_cols: for(int c = 0; c < POOL_IMG_COLS; ++c)
		{
			#pragma HLS UNROLL
			
			flat_to_dense_stream.write(pool_to_flat_stream.read());
		}
	}
}
\end{lstlisting}
\end{latin}


\زیرقسمت{ماژول اصلی (\lr{Top Module})}
تاپ ماژول طراحی به‌نام \lr{CNN} به‌صورت زیر تعریف شده است: \newpage

\begin{latin}
\begin{lstlisting}[language=C,caption={HLS Implementation of \texttt{CNN} Top Module}]

void cnn(float img_in[IMG_ROWS][IMG_COLS], float prediction[DIGITS])
{
	/******** Pre-processing data. ********/
	
	float pad_img0[PAD_IMG_ROWS][PAD_IMG_COLS] = { 0 };
	normalization_and_padding(img_in, pad_img0);
	
	#if 0
		#ifndef __SYNTHESIS__
			printf("Padded image.\n");
			print_pad_img(pad_img);
		#endif
	#endif
	
	/* Allow parallelism cloning the padded image. */
	float pad_img1[PAD_IMG_ROWS][PAD_IMG_COLS];
	float pad_img2[PAD_IMG_ROWS][PAD_IMG_COLS];
	float pad_img3[PAD_IMG_ROWS][PAD_IMG_COLS];
	
	float value;
	
	clone_for_rows: for(int i = 0; i < PAD_IMG_ROWS; ++i)
	{
		clone_for_cols: for(int j = 0; j < PAD_IMG_COLS; ++j)
		{
			pad_img1[i][j] = pad_img0[i][j];
			pad_img2[i][j] = pad_img0[i][j];
			pad_img3[i][j] = pad_img0[i][j];
		}
	}
	
	
	/* Parallel executions start here. */
	dataflow_section(pad_img0, pad_img1, pad_img2, pad_img3, prediction);
}

\end{lstlisting}
\end{latin}




\زیرقسمت{ماژول \lr{Data Flow}}
برای مشخص کردن جریان انجام محاسبات در \lr{Top Module}، ماژولی به‌نام \texttt{dataflow\_section} تعریف شده است که کد آن به‌صورت زیر ارائه می‌شود:


\begin{latin}
\begin{lstlisting}[language=C,caption={HLS Implementation of \texttt{Data flow}  Module}]

void dataflow_section( float pad_img0 [PAD_IMG_ROWS][PAD_IMG_COLS],
					   float pad_img1 [PAD_IMG_ROWS][PAD_IMG_COLS],
					   float pad_img2 [PAD_IMG_ROWS][PAD_IMG_COLS],
					   float pad_img3 [PAD_IMG_ROWS][PAD_IMG_COLS],
					   float prediction [DIGITS] )
{
	#pragma HLS DATAFLOW
	/******** Convolution layer. ********/
	/*
	An array to collect the convolution results:
	FILTERS resulting feature maps, one for each filter.
	*/
	
	hls::stream<float, IMG_ROWS * IMG_COLS>
	conv_to_pool_streams[FILTERS];
	
	// Convolution with relu as activation function.
	convolutional_layer(pad_img0, pad_img1, pad_img2, pad_img3, conv_to_pool_streams);
	
	#if 0
	#ifndef __SYNTHESIS__
	// Print results.
	print_features(conv_to_pool_streams);
	#endif
	#endif
	
	/******** Maxpooling layer. ********/
	hls::stream<float, POOL_IMG_ROWS * POOL_IMG_COLS>
	pool_to_flat_streams[FILTERS];
	
	max_pooling_layer(conv_to_pool_streams, pool_to_flat_streams);
	
	#if 0
	#ifndef __SYNTHESIS__
	print_pool_features(pool_to_flat_streams);
	#endif
	#endif
	
	/******** Flatten layer. ********/
	hls::stream<float, FLAT_SIZE / FILTERS> flat_to_dense_streams[FILTERS];
	flattening_layer(pool_to_flat_streams, flat_to_dense_streams);
	
	/******** Dense layer. ********/
	hls::stream<float, DENSE_SIZE> dense_to_softmax_streams [FILTERS];
	dense_layer(flat_to_dense_streams, dense_to_softmax_streams);
	
	/******** Softmax. ********/
	dense_layer_soft_max(dense_to_softmax_streams, prediction);
}
	
\end{lstlisting}
\end{latin}



ماژول‌های دیگری برای انجام عملیات‌هایی مانند \lr{Zero Padding}، \lr{Pooling}، \lr{Normalization} و ... تعریف شده است که به‌علت طولانی نشدن گزارش آورده نشده است.




\قسمت{سنتز مدل}
درنهایت ماژول \texttt{CNN} را سنتز می‌کنیم. گزارشات سنتز شبکه در شکل‌های «\ref{شکل:منابع مصرفی پس‌از سنتز (الف)}» و «\ref{شکل:منابع مصرفی پس‌از سنتز (ب)}» و «\ref{شکل:منابع مصرفی پس‌از سنتز (ج)}» آورده شده است.





\شروع{شکل}[ht]
\centerimg{Synthesis_Result_1.png}{12cm}
\شرح{منابع مصرفی پس‌از سنتز (الف)}
\برچسب{شکل:منابع مصرفی پس‌از سنتز (الف)}
\پایان{شکل}


\شروع{شکل}[ht]
\centerimg{Synthesis_Result_2.png}{12cm}
\شرح{منابع مصرفی پس‌از سنتز (ب)}
\برچسب{شکل:منابع مصرفی پس‌از سنتز (ب)}
\پایان{شکل}


\شروع{شکل}[ht]
\centerimg{Synthesis_Result_3.png}{11cm}
\شرح{منابع مصرفی پس‌از سنتز (ج)}
\برچسب{شکل:منابع مصرفی پس‌از سنتز (ج)}
\پایان{شکل}





\قسمت{تست مدل}
درنهایت فایل \lr{test bench} ای برای طراحی نوشته شده است که ۱۰۰ تصویر ابتدایی را از فایل تصاویر تولید شده در فاز قبل را بخواند و برای پردازش به شبکه بدهد. و خروجی شبکه میزان دقت تشخیص اعداد و زمان مصرف شده به‌ازای تمام فرایند می‌باشد.


برای مثال، شبکه ما ۱۰۰ تصویر ابتدایی از مجموعه داده‌های تست دیتاست \lr{MNIST} را با دقت ۱۰۰٪ تشخیص می‌دهد «شکل \ref{شکل:Accuracy پیش‌بینی به‌ازای ۱۰۰ تصویر}»:



\شروع{شکل}[ht]
\centerimg{Synthesis_Result_3.png}{11cm}
\شرح{\lr{Accuracy} پیش‌بینی به‌ازای ۱۰۰ تصویر}
\برچسب{شکل:Accuracy پیش‌بینی به‌ازای ۱۰۰ تصویر}
\پایان{شکل}






شبکه ما فاز \lr{Inference} را به ازای ۱۰۰ تصویر در ۴۵٫۴ میلی‌ثانیه انجام داده است. درصورتی که در فاز نرم‌افزاری همین تعداد تصویر در مدت زمان ۹۷٫۳۹ میلی‌ثانیه انجام شده است. \newpage


هرچه تعداد تصاویر را زیاد کنیم دقت شبکه کم می‌شود. برای مثال به ازای ۵۰۰ تصویر دقت تشخیص شبکه ۹۹ درصد به‌دست می‌آید.

\شروع{شکل}[ht]
\centerimg{Synthesis_Result_3.png}{11cm}
\شرح{\lr{Accuracy} پیش‌بینی به‌ازای ۵۰۰ تصویر}
\برچسب{شکل:Accuracy پیش‌بینی به‌ازای ۵۰۰ تصویر}
\پایان{شکل}
\newpage




در این حالت شبکه ۵ تصویر زیر را به اشتباه تشخیص داده است:

\شروع{شکل}[ht]
\centerimg{img11.png}{15cm}
\شرح{تشخیص‌های اشتباه شبکه}
\برچسب{شکل:تشخیص‌های اشتباه شبکه}
\پایان{شکل}




مقادیری که شبکه برای هر خروجی اشتباه به‌دست آورده است نیز به‌صورت زیر ارائه می‌شود: \newpage


\begin{latin}
	\noindent\begin{multicols}{3}
		\texttt{%
			Prediction for A:\\
			0: 0.000014\\
			1: 0.000002\\
			2: 0.000000\\
			\hl{3: 0.806685}\\
			4: 0.001617\\
			5: 0.037086\\
			6: 0.000001\\
			7: 0.000960\\
			8: 0.003560\\
			9: 0.150075
		}
		
		\vfill % Optional spacing
		
		\texttt{%
			Prediction for B:\\
			0: 0.000006\\
			1: 0.000024\\
			2: 0.078408\\
			3: 0.048098\\
			4: 0.000000\\
			5: 0.000003\\
			6: 0.000000\\
			\hl{7: 0.596224}\\
			8: 0.271232\\
			9: 0.006006
		}
		
		\vfill % Optional spacing
		
		\texttt{%
			Prediction for C:\\
			0: 0.268470\\
			1: 0.006770\\
			2: 0.000006\\
			3: 0.000007\\
			4: 0.025127\\
			5: 0.002049\\
			6: 0.006413\\
			\hl{7: 0.627097}\\
			8: 0.047987\\
			9: 0.016075
		}
	\end{multicols}
\end{latin}



\begin{latin}
	\noindent\begin{multicols}{2}
		\texttt{%
			Prediction for D:\\
			0: 0.000006\\
			1: 0.005681\\
			2: 0.000050\\
			\hl{3: 0.955743}\\
			4: 0.000271\\
			5: 0.037680\\
			6: 0.000232\\
			7: 0.000000\\
			8: 0.000259\\
			9: 0.000077
		}
		
		
		\texttt{%
			Prediction for E:\\
			0: 0.000000\\
			1: 0.000000\\
			2: 0.137538\\
			3: 0.003166\\
			4: 0.000000\\
			5: 0.000000\\
			6: 0.000000\\
			7: 0.000023\\
			\hl{8: 0.859272}\\
			9: 0.000001
		}
	\end{multicols}
\end{latin}
